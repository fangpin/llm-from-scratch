[中文](./README_cn.md)

# LLM from Scratch

This repository contains a from-scratch implementation of a Transformer model in PyTorch, built for educational purposes. It includes all the essential building blocks of a modern transformer architecture, written in a clear and understandable way.

## Features

* Custom implementations of core transformer components.
* Detailed and well-commented code.
* Includes modern techniques like RoPE, SwiGLU, and RMSNorm.
* Custom optimizers (`SGDDecay`, `AdamW`) and a learning rate scheduler.

## Implemented Components

* **Modules**: `Linear`, `Embedding`, `RmsNorm`, `SiLu`, `SwiGlu`, `FFN`, `RoPE`, `Softmax`, `ScaledDotProductAttention`, `MultiHeadAttention`, `TransformerBlock`, `Transformer`, `BpeTokenizer`.
* **Loss Function**: `CrossEntropyLoss`.
* **Optimizers**: `SGDDecay`, `AdamW`.
* **Utilities**: `cos_lr_scheduler`, `gradient_clip`.

## Architecture

The transformer model in this repository is a decoder-only model, suitable for language modeling tasks. It features:

* **Pre-Normalization**: Uses `RmsNorm` for layer normalization before the attention and feed-forward layers.
* **SwiGLU Activation**: The feed-forward network uses the SwiGLU activation function for better performance.
* **Rotary Position Embedding (RoPE)**: Incorporates positional information by rotating the query and key vectors in the attention mechanism.

## File Structure

```
lm/
├── __init__.py
├── bpe_tokenizer.py
├── checkpoint.py
├── generating.py
├── training.py
└── transformer.py
```

* `bpe_tokenizer.py`: A from-scratch implementation of the Byte Pair Encoding (BPE) tokenizer. It now correctly handles special tokens and whitespace variations, including trailing whitespaces, to align with the behavior of reference tokenizers like `tiktoken`. It also includes an `encode_iterable` method for memory-efficient tokenization of large text streams. The `train` method has been optimized for better performance.
* `transformer.py`: The core transformer model, including all the building blocks like attention, FFN, and RoPE.
* `training.py`: A script for training the transformer model on a text corpus.
* `generating.py`: A script for generating text using a trained model.
* `checkpoint.py`: Utility functions for saving and loading model checkpoints.

## Usage

### 1. Training the Tokenizer

The `bpe_tokenizer.py` script can be used to train a BPE tokenizer on your own text data.

```bash
python lm/bpe_tokenizer.py --corpus your_text_file.txt --vocab_size 10000
```

### 2. Training the Model

The `training.py` script is used to train the transformer model.

```bash
python lm/training.py \
    --train_data path/to/train_data.bin \
    --val_data path/to/val_data.bin \
    --d_model 512 \
    --num_heads 8 \
    --d_ff 2048 \
    --vocab_size 10000 \
    --num_layers 6 \
    --max_seq_len 512 \
    --batch_size 32 \
    --iterations 10000 \
    --device cuda:0
```

### 3. Generating Text

Once you have a trained model, you can use `generating.py` to generate text.

```bash
python lm/generating.py \
    --model_path path/to/your/checkpoint.pt \
    --tokenizer_path path/to/your/tokenizer.json \
    --prompt "Hello, world!" \
    --max_tokens 100 \
    --temperature 0.8 \
    --top_p 0.9 \
    --device cuda:0
```

## Installation

To use this code, you will need to have PyTorch and einx installed:

```bash
pip install torch einx
```

## Pre-requisites

* Python 3.8 or higher
* PyTorch 1.10 or higher
* einx
* regex

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Citation

If you use this code in your research, please consider citing it as follows:

```bibtex
@misc{transformer-from-scratch,
  author = {Your Name},
  title = {Transformer from Scratch},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/your-username/transformer-from-scratch}},
}
```

## Contributing

Contributions are welcome! Please feel free to submit a pull request or open an issue if you have any suggestions or find any bugs.

## Disclaimer

This implementation is for educational purposes only and may not be suitable for production use.

---

*This README was generated by gemini-cli.*

